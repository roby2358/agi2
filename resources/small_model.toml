# Small Model Configuration - Fast Training
# Usage: python agi2_train.py resources/small_model.toml

# Training data sources - list of corpus files
sources = [
    "resources/training/corpus.txt",
    "resources/training/corpus01.txt",
    "resources/training/corpus20.txt"
]

# Legacy support - kept for backward compatibility
corpus_path = "samples/corpus.txt"
model_name = "small_model"
model_path = "trained/small_model.pt"

# Training parameters - optimized for speed
epochs = 50
batch_size = 16
learning_rate = 5e-4
seq_len = 512

# Model architecture - smaller for faster training
model_positions = 512
model_embd = 384
model_layer = 6
model_head = 6

# Device and resume options
device = "auto"
resume = ""

# Generation parameters
max_length = 300
temperature = 0.9
beam_size = 3
model_seed = """
This is a short, creative text generation model. It produces concise and imaginative responses.
"""

# Interactive parameters
max_context_length = 512
