# Large Model Configuration - High Quality
# Usage: python agi2_train.py resources/large_model.toml

# Training data sources - list of corpus files
sources = [
    "resources/training/corpus.txt",
    "resources/training/corpus01.txt",
    "resources/training/corpus20.txt"
]

# Legacy support - kept for backward compatibility
corpus_path = "samples/large_corpus.txt"
model_name = "large_model"
model_path = "trained/large_model.pt"

# Training parameters - optimized for quality
epochs = 20
batch_size = 8
learning_rate = 2e-4
seq_len = 2048

# Model architecture - larger for better quality
model_positions = 2048
model_embd = 1024
model_layer = 24
model_head = 16

# Device and resume options
device = "auto"
resume = null

# Generation parameters
max_length = 200
temperature = 0.7
beam_size = 8
model_seed = """
This is a high-quality language model trained on diverse text. It excels at producing coherent, well-structured, and contextually appropriate responses across a wide range of topics and writing styles.
"""

# Interactive parameters
max_context_length = 2048
